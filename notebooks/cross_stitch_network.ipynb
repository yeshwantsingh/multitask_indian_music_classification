{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go9C3uLL8Izc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dzLKpmZICaWN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from src.data.load_data import get_dataset\n",
    "from src.data.split_data import train_val_split\n",
    "\n",
    "from src.models.cross_stitch import get_cross_stitch_network_basic\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HscIuZ2g8S0z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2-rayb7-3Y0I",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '../data/processed/mel_spec/Indian Regional Music/'\n",
    "\n",
    "languages = ['Assamese', 'Bengali', 'Gujarati', 'Hindi', 'Kannada', 'Kashmiri', 'Khasi and Jaintia', 'Konkani', 'Malayalam',\n",
    "             'Manipuri', 'Marathi', 'Nagamese', 'Nepali', 'Oriya', 'Punjabi', 'Tamil', 'Telegu']\n",
    "\n",
    "artists = ['Bhupen Hazarika', 'Dipali Barthakur', 'Tarali Sarma', 'Zubeen Garg', 'Arijit Singh', 'Geeta Dutt', 'Manna Dey',\n",
    "           'Shreya Ghoshal', 'Atul Purohit', 'Devang Patel', 'Falguni pathak', 'Geeta Rabari', 'Alka yagnik', 'Kishore Kumar',\n",
    "           'Lata Mangeshkar', 'Sonu Nigam', 'Anuradha Bhat', 'B. K. Sumitra', 'P. B. Sreenivas', 'Rajkumar', 'Raj Begum',\n",
    "           'Rashid Jahangir', 'Shameema Dev Azad', 'Waheed Jeelani', 'George Shadap', 'Rida Gatphoh', 'Soulmate',\n",
    "           'Wanshankupar Suchiang', 'Amit Naik', 'Lorna Cordeiro', 'Remo Fernandes', 'Xavier Gomes', 'K J Yesudas', 'Mohanlal',\n",
    "           'Shweta Mohan', 'Sujatha Mohan', 'Kunjabihari', 'Nongmaithem Pahari', 'Pinky Saikhom', 'Pushparani', \n",
    "           'Hridaynath Mangeshkar', 'Milind Ingle', 'Shridhar Phadke', 'Vaishali Samant', 'David Konyak', 'Mengu Suokhrie',\n",
    "           'Silas Kikon', 'Tetseo Sisters', 'Anju Panta', 'Aruna Lama', 'Narayan Gopal', 'Raju Lama', 'Akshay Mohanty',\n",
    "           'Bishnu Mohan Kabi', 'Ira Mohanty', 'Tapu Mishra', 'Gurdas Maan', 'Hans Raaj Hans', 'Jasmine Sandlas',\n",
    "           'Jaswinder Brar', 'Hariharan', 'K. S. Chithra', 'S. P. Balasubrahmanyam', 'Sistla Janaki', 'LV Revanth', 'Malavika',\n",
    "           'S S Thaman', 'S. P. Sailaja']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70IBxSKxA1N9",
    "outputId": "6ea34f52-155c-4a2a-f12d-b81041e4788e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "        'mel_spec': [],\n",
    "        'language': [],\n",
    "        'location': [],\n",
    "        'artist': [],\n",
    "        'gender': [],\n",
    "        'song': [],\n",
    "        'location_id': [],\n",
    "        'artist_id': [],\n",
    "        'gender_id': [],\n",
    "        # 'veteran': [],\n",
    "        'no_of_artists': [],\n",
    "        'song_id': [],\n",
    "        'local_song_id': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dataset = get_dataset(DATASET_PATH, dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1638.0 409.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train, val, test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_val_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_size_percentage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_size_percentage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/B/multitask_indian_music_classification/src/data/split_data.py:39\u001B[0m, in \u001B[0;36mtrain_val_split\u001B[0;34m(dataset, train_size_percentage, val_size_percentage)\u001B[0m\n\u001B[1;32m     35\u001B[0m val_size \u001B[38;5;241m=\u001B[39m (dataset_size \u001B[38;5;241m*\u001B[39m val_size_percentage) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_size, val_size)\n\u001B[0;32m---> 39\u001B[0m x_train, y1_train, y2_train, y3_train, y4_train, y5_train \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmel_spec\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43mtrain_size\u001B[49m\u001B[43m]\u001B[49m,\\\n\u001B[1;32m     40\u001B[0m                                                             dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation_id\u001B[39m\u001B[38;5;124m'\u001B[39m][:train_size],\\\n\u001B[1;32m     41\u001B[0m                                                             dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124martist_id\u001B[39m\u001B[38;5;124m'\u001B[39m][:train_size],\\\n\u001B[1;32m     42\u001B[0m                                                             dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgender_id\u001B[39m\u001B[38;5;124m'\u001B[39m][:train_size],\\\n\u001B[1;32m     43\u001B[0m                                                             dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msong_id\u001B[39m\u001B[38;5;124m'\u001B[39m][:train_size],\\\n\u001B[1;32m     44\u001B[0m                                                             dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_song_id\u001B[39m\u001B[38;5;124m'\u001B[39m][:train_size]\n\u001B[1;32m     46\u001B[0m x_val, y1_val, y2_val, y3_val, y4_val, y5_val \u001B[38;5;241m=\u001B[39m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmel_spec\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size:train_size\u001B[38;5;241m+\u001B[39mval_size],\\\n\u001B[1;32m     47\u001B[0m                                                 dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation_id\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size:train_size\u001B[38;5;241m+\u001B[39mval_size],\\\n\u001B[1;32m     48\u001B[0m                                                 dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124martist_id\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size:train_size\u001B[38;5;241m+\u001B[39mval_size],\\\n\u001B[1;32m     49\u001B[0m                                                 dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgender_id\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size:train_size\u001B[38;5;241m+\u001B[39mval_size],\\\n\u001B[1;32m     50\u001B[0m                                                 dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msong_id\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size:train_size\u001B[38;5;241m+\u001B[39mval_size],\\\n\u001B[1;32m     51\u001B[0m                                                 dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_song_id\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size:train_size\u001B[38;5;241m+\u001B[39mval_size]\n\u001B[1;32m     53\u001B[0m x_test, y1_test, y2_test, y3_test, y4_test, y5_test \u001B[38;5;241m=\u001B[39m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmel_spec\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size\u001B[38;5;241m+\u001B[39mval_size:],\\\n\u001B[1;32m     54\u001B[0m                                                       dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation_id\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size\u001B[38;5;241m+\u001B[39mval_size:],\\\n\u001B[1;32m     55\u001B[0m                                                       dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124martist_id\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size\u001B[38;5;241m+\u001B[39mval_size:],\\\n\u001B[1;32m     56\u001B[0m                                                       dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgender_id\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size\u001B[38;5;241m+\u001B[39mval_size:],\\\n\u001B[1;32m     57\u001B[0m                                                       dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msong_id\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size\u001B[38;5;241m+\u001B[39mval_size:],\\\n\u001B[1;32m     58\u001B[0m                                                       dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_song_id\u001B[39m\u001B[38;5;124m'\u001B[39m][train_size\u001B[38;5;241m+\u001B[39mval_size:]\n",
      "\u001B[0;31mTypeError\u001B[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "train, val, test = train_val_split(dataset, train_size_percentage=0.8, val_size_percentage=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "avg_loss = []\n",
    "avg_acc = []\n",
    "avg_recall = []\n",
    "avg_precision = []\n",
    "avg_specificity = []\n",
    "avg_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 16:16:44.415252: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-20 16:16:44.969006: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-06-20 16:16:44.969158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12535 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:65:00.0, compute capability: 7.5\n",
      "2022-06-20 16:16:45.100886: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 12265543680 exceeds 10% of free system memory.\n",
      "2022-06-20 16:16:53.144532: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 12265543680 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 184, in __call__\n        self.build(y_pred)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 132, in build\n        self._losses = self._conform_to_outputs(y_pred, self._losses)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 56, in _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 651, in map_to_output_names\n        raise ValueError(\n\n    ValueError: Found unexpected losses or metrics that do not correspond to any Model output: dict_keys(['output_1', 'output_2', 'output_3', 'output_4']). Valid mode output names: ['output1', 'output2', 'output3', 'output4']. Received struct is: {'output_1': <keras.losses.SparseCategoricalCrossentropy object at 0x7fd8318f5e50>, 'output_2': <keras.losses.SparseCategoricalCrossentropy object at 0x7fd8318f1c70>, 'output_3': <keras.losses.BinaryCrossentropy object at 0x7fd8318f1eb0>, 'output_4': <keras.losses.BinaryCrossentropy object at 0x7fd8318f1b20>}.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m),\n\u001B[1;32m      8\u001B[0m           loss\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput_1\u001B[39m\u001B[38;5;124m'\u001B[39m: tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mSparseCategoricalCrossentropy(from_logits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m      9\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput_2\u001B[39m\u001B[38;5;124m'\u001B[39m: tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mSparseCategoricalCrossentropy(from_logits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m                 },\n\u001B[1;32m     13\u001B[0m           metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43my_train_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_4\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m acc \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(x\u001B[38;5;241m=\u001B[39mx_test, y\u001B[38;5;241m=\u001B[39m(y_test_1, y_test_2, y_test_3, y_test_4))\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1147\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[1;32m   1146\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[1;32m   1148\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1149\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 184, in __call__\n        self.build(y_pred)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 132, in build\n        self._losses = self._conform_to_outputs(y_pred, self._losses)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 56, in _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    File \"/home/ys/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 651, in map_to_output_names\n        raise ValueError(\n\n    ValueError: Found unexpected losses or metrics that do not correspond to any Model output: dict_keys(['output_1', 'output_2', 'output_3', 'output_4']). Valid mode output names: ['output1', 'output2', 'output3', 'output4']. Received struct is: {'output_1': <keras.losses.SparseCategoricalCrossentropy object at 0x7fd8318f5e50>, 'output_2': <keras.losses.SparseCategoricalCrossentropy object at 0x7fd8318f1c70>, 'output_3': <keras.losses.BinaryCrossentropy object at 0x7fd8318f1eb0>, 'output_4': <keras.losses.BinaryCrossentropy object at 0x7fd8318f1b20>}.\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in StratifiedKFold(n_splits=10).split(dataset['mel_spec'], dataset['location_id']):\n",
    "    x_train, y_train_1, y_train_2, y_train_3, y_train_4 = dataset['mel_spec'][train_index], dataset['location_id'][train_index], dataset['artist_id'][train_index], dataset['gender_id'][train_index], dataset['no_of_artists'][train_index]\n",
    "\n",
    "    x_test, y_test_1, y_test_2, y_test_3, y_test_4 = dataset['mel_spec'][test_index], dataset['location_id'][test_index], dataset['artist_id'][test_index], dataset['gender_id'][test_index], dataset['no_of_artists'][test_index]\n",
    "\n",
    "    model = get_cross_stitch_network_basic((128, 130))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss={'output_1': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    'output_2': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    'output_3': tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                    'output_4': tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                    },\n",
    "              metrics=['accuracy'])\n",
    "    #\n",
    "    history = model.fit(x=x_train, y=(y_train_1, y_train_2, y_train_3, y_train_4), epochs=1, verbose=0)\n",
    "    acc = model.evaluate(x=x_test, y=(y_test_1, y_test_2, y_test_3, y_test_4))\n",
    "    # # avg_loss.append(loss)\n",
    "    # avg_acc.append(acc)\n",
    "    # avg_recall.append(rec)\n",
    "    # avg_precision.append(pre)\n",
    "    # avg_specificity.append(spec)\n",
    "    # avg_f1.append(f1_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZTt3kO3mfm4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluate the model performance\n",
    "\n",
    "Run the model on the test set and check the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nBSQYFfUW5G",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(history.epoch, metrics['loss'])\n",
    "plt.plot(history.epoch, metrics['output1_loss'])\n",
    "plt.plot(history.epoch, metrics['output2_loss'])\n",
    "plt.plot(history.epoch, metrics['output3_loss'])\n",
    "plt.plot(history.epoch, metrics['output4_loss'])\n",
    "plt.plot(history.epoch, metrics['output5_loss'])\n",
    "plt.plot(history.epoch, metrics['val_output1_loss'])\n",
    "plt.plot(history.epoch, metrics['val_output2_loss'])\n",
    "plt.plot(history.epoch, metrics['val_output3_loss'])\n",
    "plt.plot(history.epoch, metrics['val_output4_loss'])\n",
    "plt.plot(history.epoch, metrics['val_output5_loss'])\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.epoch, metrics['output1_accuracy'])\n",
    "plt.plot(history.epoch, metrics['output2_accuracy'])\n",
    "plt.plot(history.epoch, metrics['output3_accuracy'])\n",
    "plt.plot(history.epoch, metrics['output4_accuracy'])\n",
    "plt.plot(history.epoch, metrics['output5_accuracy'])\n",
    "plt.plot(history.epoch, metrics['val_output1_accuracy'])\n",
    "plt.plot(history.epoch, metrics['val_output2_accuracy'])\n",
    "plt.plot(history.epoch, metrics['val_output3_accuracy'])\n",
    "plt.plot(history.epoch, metrics['val_output4_accuracy'])\n",
    "plt.plot(history.epoch, metrics['val_output5_accuracy'])\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist[['loss', 'output1_loss', 'output2_loss', 'output3_loss', 'output4_loss',\n",
    "       'output5_loss', 'val_loss', 'val_output1_loss', 'val_output2_loss', 'val_output3_loss',\n",
    "       'val_output4_loss', 'val_output5_loss',]].plot(figsize=(20, 10))\n",
    "plt.legend(loc=9, ncol=8)\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hist[['output1_accuracy', 'output2_accuracy', 'output3_accuracy', 'output4_accuracy',\n",
    "      'output5_accuracy','val_output2_accuracy', 'val_output3_accuracy', 'val_output4_accuracy',\n",
    "       'val_output5_accuracy']].plot(figsize=(20, 10))\n",
    "plt.legend(loc=9, ncol=7)\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_label1 = np.concatenate([y1 for mel_spec, (y1, y2, y3, y4, y5) in test_ds], axis=0)\n",
    "test_label2 = np.concatenate([y2 for mel_spec, (y1, y2, y3, y4, y5) in test_ds], axis=0)\n",
    "test_label3 = np.concatenate([y3 for mel_spec, (y1, y2, y3, y4, y5) in test_ds], axis=0)\n",
    "test_label4 = np.concatenate([y4 for mel_spec, (y1, y2, y3, y4, y5) in test_ds], axis=0)\n",
    "test_label5 = np.concatenate([y5 for mel_spec, (y1, y2, y3, y4, y5) in test_ds], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3X3XWk8UW5I",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred1, y_pred2, y_pred3, y_pred4, y_pred5 = model.predict(test_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_true1 = test_label1\n",
    "y_true2 = test_label2\n",
    "y_true3 = test_label3\n",
    "y_true4 = test_label4\n",
    "y_true5 = test_label5\n",
    "\n",
    "y_pred1 = np.argmax(y_pred1, axis = -1)\n",
    "y_pred2 = np.argmax(y_pred2, axis = -1)\n",
    "y_pred3 = np.argmax(y_pred3, axis = -1)\n",
    "y_pred4 = np.argmax(y_pred4, axis = -1)\n",
    "y_pred5 = np.argmax(y_pred5, axis = -1)\n",
    "\n",
    "test_acc1 = sum(y_pred1 == y_true1) / len(y_true1)\n",
    "print(f'Test set accuracy for label1: {test_acc1:.0%}')\n",
    "\n",
    "test_acc2 = sum(y_pred2 == y_true2) / len(y_true2)\n",
    "print(f'Test set accuracy for label2: {test_acc2:.0%}')\n",
    "\n",
    "test_acc3 = sum(y_pred3 == y_true3) / len(y_true3)\n",
    "print(f'Test set accuracy for label3: {test_acc3:.0%}')\n",
    "\n",
    "test_acc4 = sum(y_pred4 == y_true4) / len(y_true4)\n",
    "print(f'Test set accuracy for label4: {test_acc4:.0%}')\n",
    "\n",
    "test_acc5 = sum(y_pred5 == y_true5) / len(y_true5)\n",
    "print(f'Test set accuracy for label5: {test_acc5:.0%}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Znt1NOabH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Display a confusion matrix\n",
    "\n",
    "Use a <a href=\"https://developers.google.com/machine-learning/glossary#confusion-matrix\" class=\"external\">confusion matrix</a> to check how well the model did classifying each of the langs in the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZqpUobUUW5J",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true1, y_pred1)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            # xticklabels=languages,\n",
    "            # yticklabels=languages,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3qxwIYgtwUp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true2, y_pred2)\n",
    "plt.figure(figsize=(25, 15))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            # xticklabels=artists,\n",
    "            # yticklabels=artists,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true3, y_pred3)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            # xticklabels=artists,\n",
    "            # yticklabels=artists,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label3')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true5, y_pred5)\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            # xticklabels=artists,\n",
    "            # yticklabels=artists,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label5')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MTL2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}